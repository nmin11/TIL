_스스로 움직이는 사용자 에이전트_

- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 웹 사이트들을 떠돌아다니면서 컨텐츠를 가져오고 하이퍼링크를 따라가고 발견한 데이터를 처리함
- '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불림

※ 예시

- 주식시장 서버에 매 분 GET 요청을 보내서 얻은 데이터로 주가 추이 그래프를 생성하는 주식 그래프 로봇
- WWW의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
  - 웹 페이지 개수를 세고 각 페이지의 크기, 언어, 미디어 타입을 기록함
  - http://www.netcraft.com → 웹 사이트들이 어떤 서버를 사용하고 있는지 통계 자료를 수집함
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
- 상품 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹 페이지를 수집하는 가격 비교 로봇

<br>

## 1. 크롤러와 크롤링

- 웹 페이지를 가져온 뒤, 그 페이지가 가리키는 모든 웹 페이지를 가져오고, 또 그 페이지의 페이지들을 가져오는,<br>웹을 재귀적으로 반복 순회하는 로봇
- 크롤러 혹은 스파이더라고 불림
  - HTML 하이퍼링크들로 만들어진 웹을 따라 '기어다니기' 때문
- 인터넷 검색엔진은 웹을 돌아다니며 만나는 모든 문서를 끌어오기 위해 크롤러를 사용함
  - 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어짐
  - 검색 DB는 특정 단어를 포함한 문서를 찾을 수 있게 해줌
  - 찾아서 가져와야 하는 페이지들이 수십억 개에 이르다 보니 필연적으로 가장 복잡한 로봇들 중 하나가 되었음

### 1.1 어디서 시작하는가: '루트 집합'

- 굶주린 크롤러를 풀어놓기 전에 출발지점이 주어져야 함

![크롤러 루트 집합](https://user-images.githubusercontent.com/75058239/202581426-610bccd3-0b18-4fec-9cc6-21bacafe4a8b.png)

- 크롤러가 방문을 시작하는 URL들의 초기 집합을 **root set**이라고 부름
- 루트 집합을 고를 땐 관심 있는 웹 페이지들을 충분히 크롤링하기 위해 충분히 다른 장소의 URL들을 선택해야 함
- 일반적으로 웹 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없음
- 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지 목록,<br>자주 링크되지만 잘 알려져 있지 않은 페이지 목록으로 구성되어 있음
- 대규모 크롤러 제품들은 사용자들이 루트 집합에 새로운 페이지나 잘 알려지지 않은 페이지들을 추가하는 기능을 제공함
- 루트 집합은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 됨

### 1.2 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 페이지 안의 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함
- 크롤러는 간단한 HTML 파싱을 통해 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있음

### 1.3 순환 피하기

- 로봇이 웹을 크롤링할 때 루프에 빠지지 않도록 매우 조심해야 함
- 만약 A에서 출발해서, B를 거쳐서 C에 도착했는데 C에서 A로 다시 링크되면 무한 루프에 빠지게 될 것
- 이를 해결하기 위해 로봇들은 반드시 어디에 방문했었는지를 알아야 함

### 1.4 루프와 중복

루프가 크롤러에게 해로운 3가지 이유

1. 크롤러가 네트워프 대역폭을 다 차지하면서도 동일한 페이지들만을 반복해서 가져오게 될 수 있음
2. 웹 서버에 불필요한 부담을 가중시켜서 다른 사용자의 사이트 접근을 막을 수도 있게 되는데,<br>이는 서비스 방해 행위로써 법적인 문제가 될 수 있음
3. 크롤러의 애플리케이션이 쓸모없는 중복된 컨텐츠로 가득차게 됨

### 1.5 빵 부스러기의 흔적

- 불행하게도 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음
  - 동적인 게이트웨이에서 생성된 컨텐츠를 제외하더라도 수십억 개의 서로 다른 페이지들이 있기 때문
- 만약 전 세계 웹 컨텐츠의 대부분을 크롤링하고 싶다면 수십억 개의 URL을 방문할 준비가 되어 있어야 함
- 따라서 어떤 URL을 방문했는지 빠르게 판단하기 위해 복잡한 자료 구조를 사용할 필요가 있음
- 자료 구조 중에서도 빠른 속도를 위해 적어도 검색 트리나 해시 테이블이 필요할 것
- 만약 URL의 평균 길이가 40Byte이고, 대강 5억 개의 URL을 크롤링한다면 20GB 이상의 메모리가 필요함

대규모 웹 크롤러들이 사용하는 유용한 기법들

**트리와 해시 테이블**

- URL을 훨씬 빨리 찾게 해주는 소프트웨어 자료구조

**느슨한 존재 비트맵**

- 공간 최적화를 위해 몇몇 대규모 크롤러는 _presence bit array_ 같은 느슨한 자료 구조를 사용함
- 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 presence bit를 가짐
- URL이 크롤링되었을 때, 해당 URL에 대한 존재 비트가 생성됨
- 만약 존재 비트가 이미 존재한다면 크롤러는 해당 URL이 이미 크롤링되었다고 간주함
- 하지만 잠재적으로 무한한 URL 개수에 비해 유한한 존재 비트 배열을 사용하기 때문에 충돌이 발생할 수 있고,<br>충돌로 인한 패널티로 해당 URL은 크롤링에서 제외될 수 있음

**체크포인트**

- 로봇 프로그램의 갑작스런 중단에 대비해서, 방문한 URL 목록을 디스크에 저장해두고 확인함

**파티셔닝**

- 웹의 성장에 따라, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌음
- 몇몇 대규모 웹 로봇은 여러 컴퓨터의 로봇들이 동시에 일하는 _farm_ 을 운영함
- 각 로봇들은 URL들의 특정 한 부분을 담당하며, 서로를 도우며 웹을 크롤링함
  - 서로 URL들을 넘겨주거나, 오동작하는 동료를 도와주거나, 활동을 조정하기 위해 커뮤니케이션함

### 1.6 alias와 로봇 순환

- URL은 별칭을 가질 수 있기 때문에 어떤 페이지를 이미 방문했는지 구분하는 게 쉽지 않을 수 있음

※ 예시

- 포트가 생략된 URL과 생략되지 않은 URL
- `%7F`가 `~`라는 문자열을 표현하듯, 표현 방식이 다를 때
- `#`이 붙는 태그가 있긴 한데, 이에 따라 페이지가 변동되지 않는 경우
- 서버가 URL의 대소문자를 구분하지 않는 경우
- 기본 페이지 `index.html`이 생략된 경우와 생략되지 않은 경우
- IP 주소와 도메인 주소가 다르지만 같은 리소스를 가리키고 있을 때

### 1.7 URL 정규화하기

- 대부분의 웹 로봇은 '정규화'를 통해, 다른 URL이지만 같은 리소스를 가리키는 것들을 미리 제거하려고 시도함

※ URL의 정규화 방식

1. 포트가 명시되어 있지 않다면 `:80` 추가
2. `%`로 시작하는 이스케이핑 문자들은 대응되는 실제 문자들로 변환
3. `#`으로 시작하는 태그들은 제거

※ 한계

- 웹 서버가 대소문자를 구분하는지 모름
- `index.html` 같은 경로가 생략되는지 알기 위해 웹 서버의 색인 페이지 설정을 알아야 함
- IP 주소와 도메인 주소가 같은 리소스를 가리키는지 알기 위해,<br>같은 물리적 컴퓨터를 참조하는지 뿐만 아니라, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지도 알아야 함

### 1.8 파일 시스템 링크 순환

![심볼릭 링크](https://user-images.githubusercontent.com/75058239/202850924-d4f9e907-42ee-4ec3-aa1a-eb212ab97fac.jpeg)

- 파일 시스템의 심볼릭 링크는 사실상 아무것도 존재하지 않으면서, 끝없이 깊어지는 디렉토리 계층을 만들 수 있음
- 심볼릭 링크는 보통 실수에 의해 만들어지지만, 가끔 '사악한 웹 마스터'가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 함

### 1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들은 손쉽게 크롤러 루프를 만들어낼 수 있음
- 평범한 파일이지만 사실은 게이트웨이 애플리케이션인 URL을 만드는 것은 쉬운 일
- 웹 마스터에게 나쁜 뜻이 없었음에도 심볼릭 링크나 동적 컨텐츠를 통한 크롤러 함정을 만드는 경우도 있을 수 있음

### 1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법이란 없으며, 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 함
- 일반적으로 자율적인 크롤러일수록 더 쉽게 곤란한 상황에 부딪힘
- 로봇 구현자는 휴리스틱을 활용해서 문제를 피하는 데에 도움을 줄 수도 있지만,<br>이는 유효한 컨텐츠를 걸러버릴 수도 있다는 '손실'을 감수해야 하는 일

※ 웹에서 로봇이 올바르게 동작하기 위해 사용하는 기법들

**URL 정규화**

- URL을 표준 형태로 변환
- 같은 리소스를 가리키는 중복 URL 일부 회피

**너비 우선 크롤링**

- 크롤링 할 수 있는 URL들의 큰 집합을 너비 우선으로 스케줄링하면서 순환의 영향을 최소화
- 만약 로봇을 깊이 우선 방식으로 운용한다면 순환을 건드렸을 때 걷잡을 수 없게 될 것

**스로틀링**

- 특정 시간당 가져올 수 있는 페이지 수 제한

**URL 크기 제한**

- 크기 제한을 걸어둔 로봇들은 보통 1KB가 넘는 URL이라면 크롤링을 거부함
- 순환으로 인해 URL이 계속해서 길어지는 경우라면, 크기 제한으로 방지할 수 있을 것
- 이 기법을 사용하면 가져오지 못하게 되는 컨텐츠들이 분명히 있을 거라는 점에 유의해야 함

**URL/사이트 블랙리스트**

- 순환을 만들거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어서 관리하는 방법
- 사람의 손을 필요로 하지만, 오늘날 대부분의 대규모 크롤러들은 블랙리스트를 가지고 있음
- 블랙리스트는 크롤링 되는 것을 싫어하는 특정 사이트들을 피하기 위해서도 사용될 수 있음

**패턴 발견**

- 심볼릭 링크 등의 오설정들은 일정 패턴을 따르는 경향이 있음
- 예를 들면 중복 구성 요소와 함께 점점 길어질 수 있음
- 패턴이 발견되면 로봇은 URL을 잠재적 순환으로 보고 크롤링을 거절함

**컨텐츠 fingerprint**

- 복잡한 웹 크롤러들이 사용하는 '직접적인' 방법
- 이 방법을 사용하는 로봇들은 페이지의 컨텐츠에서 몇 Byte를 얻어내서 checksum을 계산함
- 만약 이전에 봤던 체크섬을 또다시 가져오게 된다면, 해당 링크는 크롤링하지 않음
- 체크섬 함수는 내용이 달라도 체크섬이 똑같게 나올 확률이 가능한 한 적은 것으로 사용해야 함
  - MD5와 같은 메시지 요약 함수가 인기 있음
- 한계
  - 동적으로 페이지가 수정되는 웹 서버들은 체크섬 계산에서 빠뜨릴 수 있음
  - 페이지 컨텐츠를 임의로 커스터마이징할 수 있는 경우, 서버의 동적인 동작이 중복 감지를 방해할 수 있음

**사람의 모니터링**

- 웹은 거친 곳, 로봇은 결국 자신이 가진 어떤 기법으로도 해결할 수 없는 문제에 봉착하게 될 것
- 모든 상용 수준 로봇은 사람이 쉽게 모니터링해서 특이사항을 바로 알 수 있도록, 진단과 로깅을 포함하도록 설계해야 함

<br>

## 2. 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않기 때문에 HTTP 명세를 지켜야 함
  - HTTP 요청을 만들며, 스스로를 HTTP/1.1 클라이언트라고 내세우기 때문에 적절한 HTTP 요청 헤더를 사용해야 함
- 많은 로봇들이 HTTP를 최소한으로만 구현하고자 함
  - 이 때문에 HTTP/1.0 요청을 보내는 로봇들이 많음

### 2.1 요청 헤더 식별하기

- HTTP를 최소한으로 지원하더라도, 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송해야 함
  - 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더들을 웹 사이트에게 보내줘야 함
- 이는 서버 측에서 잘못된 크롤러 사용자 검증과, 로봇이 다룰 수 있는 컨텐츠에 대한 정보를 얻기에 유용하게 해줌

**User-Agent**

- 서버에게 요청을 만든 로봇의 이름을 말해줌

**From**

- 로봇 사용자/관리자의 이메일 주소 제공(RFC 822 이메일 주소 포맷)

**Accept**

- 서버가 어떤 미디어 타입을 줄 수 있는지 말해줌

**Referer**

- 현재 요청 URL을 포함한 문서 URL 제공
- 서버로 하여금 어디에서 웹 사이트 링크를 찾아낼 수 있었는지에 대한 단서를 제공해줌

### 2.2 가상 호스팅

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있음
- 가상 호스팅이 널리 퍼져있기 때문에, Host 헤더가 없으면 로봇은 URL에 대한 잘못된 컨텐츠를 찾게 됨
  - 이 때문에 HTTP/1.1은 Host 헤더 사용을 요구함

<img width="748" alt="Host 헤더의 부재" src="https://user-images.githubusercontent.com/75058239/202880621-73f0612f-340e-4a60-94cb-f78e81d727e2.png">

### 2.3 조건부 요청

- 수십억 개의 웹 페이지를 받아야 하는 인터넷 검색엔진 로봇의 경우, 오직 변경되었을 때만 컨텐츠를 가져오도록 할 수 있음
- 로봇은 받아둔 마지막 버전 이후 업데이트 사항이 있는지 알아보는 조건부 HTTP 요청을 구현함
  - HTTP 캐시가 리소스의 로컬 사본에 대한 유효성을 검사하는 방법과 매우 비슷

### 2.4 응답 다루기

- 사실 대다수 로봇들은 GET 메소드 위주로 사용하기 때문에 딱히 응답을 다룰 일이 없긴 함
- 그러나 조건부 요청을 사용하거나, 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 HTTP 응답을 다룰 줄 알아야 함

**_상태 코드_**

- 로봇들은 최소한 일반적인 상태 코드들은 다룰 수 있어야 함
  - 특히 200 OK, 404 Not Found
- 로봇이 명시적으로 이해할 수 없는 상태 코드는, 해당 상태 코드가 속한 분류에 근거해서 다뤄야 함
- 모든 서버가 항상 적절한 에러 코드를 반환하지 않는다는 점에 유의해야 함
  - 에러를 기술하는 메시지를 200 OK로 응답하는 서버도 있음
  - 이에 대비해서 뭔가 할 수 있는 일은 거의 없지만 명세를 구현하는 개발자라면 알아두고 있어야 함

**_엔티티_**

- HTTP 헤더에 임베딩된 정보를 따라서 로봇들은 엔티티 자체의 정보를 찾을 수 있음
- 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대한 컨텐츠 저자가 포함시킨 정보
- http-equiv 태그 자체는 컨텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단

```html
<meta http-equiv="Refresh" content="1; URL=index.html" />
```

※ Refresh 헤더를 HTML 내에서 제공해준다.

- http-equiv 태그를 헤더로 포함시키는 서버가 있는 반면, 그렇지 않은 서버도 있음
- 로봇 구현자는 HTML 문서의 HEAD 태그에서 http-equiv 정보를 찾고자 할 수 있음

### 2.5 User-Agent 타게팅

- 웹 관리자들은 로봇들로부터의 요청을 예상해야 함
- 서버에서 브라우저의 종류에 맞게 컨텐츠를 최적화하는 경우,<br>로봇에게 "your browser does not support frames"라는 에러 문구를 전송하게 될 수 있음
- 그러므로 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함
  - 몇몇 브라우저에 특화된 컨텐츠를 개발하는 대신, 보다 다양한 클라이언트에게 대응할 수 있는 유연한 페이지 개발
  - 최소한 로봇이 방문했는데 컨텐츠를 못 얻게 되는 일이 없도록 대비해야 함

<br>

## 3. 부적절하게 동작하는 로봇들

**폭주하는 로봇**

- 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있음
  - 게다가 빠른 네트워크 연결을 갖춘 빠른 컴퓨터에서 동작하고 있을 가능성이 높음
- 빠른 속도를 가졌기 때문에 논리적 에러나 순환에 빠졌을 때는 웹 서버에 극심한 과부하를 안겨주게 될 수도 있음
- 그렇기 때문에 로봇 구현자들은 폭주 방지를 위한 보호 장치를 마련해야만 함

**오래된 URL**

- 로봇은 URL의 목록을 방문하는데, 해당 목록이 오래되었을 수 있음
- 웹 사이트의 컨텐츠가 많이 바뀌었다면, 로봇은 존재하지 않는 URL에 대한 요청을 많이 보내게 될 수도 있음
- 이 또한 에러 페이지 제공으로 인한 서버 부하로 이어질 수 있기 때문에 웹 사이트 관리자의 짜증을 유발함

**길고 잘못된 URL**

- 순환이나 로직상의 오류로 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청하게 될 수도 있음
- 이는 웹 서버 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채워버리고, 허술한 웹 서버라면 고장을 낼 수도 있음

**호기심이 지나친 로봇**

- 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어서 인터넷 검색엔진에서도 접근할 수 있도록 만들 수도 있음
  - 최악의 경우 사생활 침해라고 여겨질 수 있음
- 보통은 사적 컨텐츠에 대한 하이퍼링크를 잘 감춰두지 않아서 접근이 되는 것이긴 한데,<br>매우 광적인 로봇은 하이퍼링크가 명시적으로 존재하지도 않는 문서들을 디렉토리의 컨텐츠를 가져오는 등의 방법으로 찾아내기도 함
- 로봇 구현자는 비밀번호 파일, 신용카드 정보 등 민감한 데이터를 로봇이 검색하지 않도록 주의해야 함
- 사실, 인터넷에 링크가 존재하는 한 진정한 의미의 사적인 리소스는 거의 없음

**동적 게이트웨이 접근**

- 로봇들은 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아님
- 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청하게 될 수도 있음
  - 이 경우 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 듦
- 많은 웹 사이트 관리자들은 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇들을 좋아하지 않음

<br>

## 4. 로봇 차단하기

**"Robots Exclusion Standard"** 표준

⇒ 1994년, 로봇 커뮤니티는 웹 마스터가 로봇의 동작을 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법을 제안했음  
⇒ 로봇 접근 제어 정보를 저장하는 파일의 이름을 따서 그냥 `robots.txt`라고 불림

- 웹 서버는 서버의 문서 루트에서 `robots.txt` 파일을 제공할 수 있음
- 이 파일은 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있음
- 로봇이 이 자발적인 표준안에 따른다면, 웹 사이트의 리소스에 접근하기 전에 우선 사이트의 `robots.txt`를 요청하게 될 것

### 4.1 로봇 차단 표준

- 로봇 차단 표준은 임시방편으로 마련된 표준
  - 작성 당시 표준을 소유하고 있는 주체가 없었고 업체들은 표준의 부분집합을 제각각 구현했음
- 그래도 없는 것보다 훨씬 낫고, 대부분의 주류 업체들 및 검섹엔진 크롤러들은 이 차단 표준을 지원함
- 로봇 차단 표준에는 3가지 버전이 존재함

| 버전 | 설명                                                                        |    날짜     |
| :--: | :-------------------------------------------------------------------------- | :---------: |
| 0.0  | Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘       | 1994년 6월  |
| 1.0  | Allow 지시자를 추가 지원하는 마틴 코스터의 IETF 초안                        | 1996년 11월 |
| 2.0  | 확장 표준(정규식, 타이밍)을 포함한 숀 코너의 확장이지만, 널리 지원되지 않음 | 1996년 11월 |

- 오늘날 대부분의 로봇들은 v0.0이나 v1.0 표준을 채택하며, v2.0은 복잡하고 널리 채택되지 못하고 있음

### 4.2 웹 사이트와 robots.txt 파일들

- 로봇은 웹 사이트 방문 전에, robots.txt 파일이 존재한다면 반드시 그 파일을 가져와서 처리해야 함
- 호스트 명과 포트번호에 의해 정의되는 웹 사이트는 단 하나의 robots.txt 파일을 가짐
  - 만약 웹 사이트가 가상 호스팅된다면 각각의 가상 docroot에 서로 다른 robots.txt가 있을 수 있음
- robots.txt 파일을 서브 디렉토리에 설치할 수 있는 방법은 존재하지 않으니, 컨텐츠에 대한 차단 규칙을 종합적으로 기술해야 함

**_robots.txt 가져오기_**

- 여느 파일들과 마찬가지로 HTTP GET 메소드를 사용해서 robots.txt 파일을 가져옴
- 파일이 존재한다면 서버는 파일을 text/plain 본문으로 반환
- 파일이 없다면 로봇의 접근을 제한하지 않는 것으로 간주하고 어떤 파일이든 요청
- 로봇은 From이나 User-Agent 헤더를 통해 신원 정보와 연락처를 제공해야 함

```http
GET /robots.txt HTTP/1.0
Host: www.joes-hardware.com
User-Agent: Slurp/2.0
Date: Wed Oct 3 20:22:48 EST 2001
```

**_응답 코드_**

- 많은 웹 사이트가 robots.txt를 갖고 있지 않지만 로봇은 그 사실을 모름
- 로봇은 어떤 웹 사이트든 반드시 robots.txt 파일을 찾아봄

※ 응답 상태 코드별 로봇의 동작

- 2XX: 반드시 응답의 컨텐츠를 파싱하여 차단 규칙을 얻고, 해당 규칙에 따라야 함
- 404: 활성화된 차단 규칙이 존재하지 않으므로 제약 없이 사이트에 접근 가능
- 401, 403: 사이트로의 접근이 완전히 제한되어 있음
- 503: 사이트에 대한 리소스 검색을 뒤로 미뤄야 함
- 3XX: 리소스가 발견될 때까지 리다이렉트를 따라가야 함

### 4.3 robots.txt 파일 포맷

- 매우 단순한 줄 기반 문법을 가짐
- 각 줄은 빈 줄, 주석 줄, 규칙 줄의 3가지 종류로 이루어짐

```http
# 이 robots.txt 파일은 Slurp와 Webcrawler가 우리 사이트의 공개된 영역을 크롤링하는 것을 허락한다.

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

- 각 줄들은 레코드로 구분되며, 각 레코드는 특정 로봇 집합에 대한 차단 규칙의 집합을 기술함
  - 이 방법을 통해 로봇별로 각각 다른 차단 규칙을 적용할 수 있음
- 각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 end-of-file 문자로 끝남
- 레코드는 영향을 받는 로봇을 지정하는 하나 이상의 User-Agent 줄로 시작하며,<br>해당 로봇들이 접근할 수 있는 URL들을 말해주는 Allow 줄과 Disallow 줄이 옴
- 현실적인 이유로 로봇 소프트웨어는 CR, LF, CRLF 줄바꿈 문자를 모두 지원해야 함

**_User-Agent 줄_**

- 각 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작함

```http
User-Agent: <robot-name>
```

```http
User-Agent: *
```

- 로봇의 이름은 로봇의 HTTP GET 요청 안의 User-Agent 헤더를 통해 보내짐
- robots.txt 파일을 처리하는 로봇은 다음 레코드에 반드시 복종해야 함
  - User-Agent가 자신 이름의 부분 문자열이 될 수 있는 레코드들 중 첫 번째 것
  - User-Agent가 `*`인 레코드들 중 첫 번째 것
  - 둘 다 찾지 못했다면 어떠한 접근 제한도 없는 것
- 로봇 이름을 대소문자를 구분하지 않는 부분 문자열과 비교하므로 의도치 않게 맞는 경우에 주의해야 함

**_Disallow와 Allow 줄들_**

- User-Agent 바로 다음 줄에 위치
- 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술함
- 로봇은 반드시 요청하고자 하는 URL을 모든 Disallow와 Allow 규칙에 순서대로 맞춰 보아야 함
- 규칙 경로는 대소문자를 구분하는 접두어이기 때문에, `Disallow: /tmp`는 다음의 모든 URL에 해당함

```
http://www.joes-hardware.com/tmp
http://www.joes-hardware.com/tmp/
http://www.joes-hardware.com/tmp/pliers.html
http://www.joes-hardware.com/tmpspc/stuff.txt
```

**_Disallow/Allow 접두 매칭(prefix matching)_**

- 경로 시작 부분부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 대소문자 차이도 없이 같아야 함
- `*`는 특별한 의미를 갖지 않지만, 빈 문자열을 이용하면 모든 문자열에 매치되도록 할 수 있음
- 규칙 경로나 URL 경로의 이스케이핑된 문자들은 비교 전에 원래대로 복원됨
  - 다만 예외로 `/`를 의미하는 `%2F`는 반드시 그대로 매치되어야 함

※ 예시

|    규칙 경로     |     URL 경로     | 매치 여부 | 부연 설명                                                 |
| :--------------: | :--------------: | :-------: | :-------------------------------------------------------- |
|       /tmp       |       /tmp       |     O     | 규칙 경로 == URL 경로                                     |
|       /tmp       |  /tmpfile.html   |     O     | 규칙 경로가 URL 경로의 접두어                             |
|       /tmp       |   /tmp/a.html    |     O     | 규칙 경로가 URL 경로의 접두어                             |
|      /tmp/       |       /tmp       |     X     | 접두어가 될 수 없음                                       |
|                  |    README.txt    |     O     | 빈 문자열은 모든 것에 매치됨                              |
|  /~fred/hi.tml   | /%7Efred/hi.html |     O     | %7E 는 ~ 와 같게 취급                                     |
| /%7Efred/hi.html |  /~fred/hi.tml   |     O     | %7E 는 ~ 와 같게 취급                                     |
| /%7efred/hi.html | /%7Efred/hi.html |     O     | 이스케이핑된 문자는 대소문자를 구분하지 않음              |
|  /~fred/hi.html  | /~fred%2Fhi.html |     X     | %2F 는 / 가 맞지만 예외적으로 정확히 매칭되어야 하는 문자 |

- 접두 매칭은 꽤 잘 동작하지만 표현력이 충분치 못한 경우가 몇 가지 있음
  - 특정 경로 밑이 아니라, 특정 디렉토리 이름으로 제한하고 싶은 경우를 표현할 수단이 없음

### 4.4 그 외에 알아둘 점

robots.txt 파일을 파싱할 때 추가적으로 지켜야 할 규칙들

- robots.txt 파일은 명세가 발전함에 따라 User-Agent, Disallow, Allow 외 다른 필드를 포함할 수 있게 되었음
  - 로봇은 이해할 수 없는 필드가 있다면 무시해야 함
- 하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않음
- 주석은 파일의 어디에서든 허용됨
- 로봇 차단 표준 버전 0.0은 Allow 줄을 지원하지 않았음
  - 아직도 몇몇 로봇은 0.0 버전의 명세만 구현하고 Allow 줄들은 무시하며, 이에 따라 허용되는 URL도 탐색하지 않게 될 수 있음

### 4.5 robots.txt의 캐싱과 만료

- 로봇은 매 파일 접근마다 robots.txt를 가져올 것이 아니라, 주기적으로 가져온 결과를 캐시해야 함
- robots.txt 파일의 캐싱을 제어하기 위해 표준 HTTP 캐시 제어 메커니즘이 원 서버와 로봇 양쪽 모두에 의해 사용됨
  - 로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 함
- HTTP/1.1 클라이언트가 아닌 크롤러들이 많으므로, 웹 마스터는 크롤러가 캐시 지시자를 이해하지 못할 수 있다는 점에 주의해야 함
- 로봇 명세 초안은 Cache-Control 지시자가 존재하는 경우 7일간 캐싱하도록 하고 있음
  - 하지만 실무에서 보면 너무 긴 시간
  - 웹 서버에서 로봇 방문 때마다 robots.txt를 새로 만들어 응답해주는 경우, 이 새로운 파일을 적용할 수 없을 것

### 4.6 robots.txt 예시

```http
# 메리의 골동품 상점 웹 사이트의 robots.txt

# 수지의 로봇은 모든 동적 URL을 이해할 수 없으므로, 그것들에는 접근할 수 없게 한다.
# 메리가 수지를 위해 예약해둔 작은 영역을 제외한 모든 사적인 데이터에도 접근할 수 없게 한다.

User-Agent: Suzy-Spider
Disallow: /dynamic
Allow: /private/suzy-stuff
Disallow: /private

# 가구 탐색 로봇은 메리의 골동품 상점에서도 가구 재고 관리 프로그램을 이해할 수 있으므로 해당 리소스들을 허용해준다.
# 그 외 다른 동적 리소스와 사적인 데이터에는 접근할 수 없게 한다.

User-Agent: Furniture-Finder
Allow: /dynamic/check-inventory
Disallow: /dynamic
Disallow: /private

# 그 외 나머지는 동적 게이트웨이와 사적인 데이터에 접근할 수 없게 한다.

User-Agent: *
Disallow: /dynamic
Disallow: /private
```

### 4.7 HTML 로봇 제어 META 태그

- HTML 페이지의 저자는 로봇이 개별 페이지에 접근하는 것을 제한하기 위한 좀 더 직접적인 방법을 가지고 있음
  - HTML 문서에 직접 로봇 제어 태그를 추가하는 방법
- 이 방법은 robots.txt 표준과 마찬가지로 따를 것을 권장하지만 강제하지는 않음

```html
<meta name="robots" content="directive-list" />
```

**_로봇 META 지시자_**

- 시간이 지남에 따라 검색엔진과 로봇의 기능이 확장되었고, 그렇기 때문에 새로운 지시자가 추가될 가능성도 높음

**NOINDEX**

```html
<meta name="robots" content="NOINDEX" />
```

- 로봇에게 이 페이지를 처리하지 말고 무시하라고 말해줌
- 이 페이지의 컨텐츠를 색인이나 DB에 포함시키지 말 것

**NOFOLLOW**

```html
<meta name="robots" content="NOFOLLOW" />
```

- 로봇에게 이 페이지가 링크한 페이지를 크롤링하지 말라고 말해줌

※ 위 2개가 가장 많이 사용됨

**INDEX**

- 로봇에게 이 페이지의 컨텐츠를 인덱싱해도 된다고 말해줌

**FOLLOW**

- 로봇에게 이 페이지가 링크한 페이지를 크롤링해도 된다고 말해줌

**NOARCHIVE**

- 로봇에게 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안 된다고 말해줌
- 구글 검색엔진을 운영하는 사람들이 구글에서 그들의 컨텐츠를 캐시해서 제공하지 못하도록 구현한 것
  - `meta name="googlebot"`과 함께 사용될 수 있음

**ALL**

- INDEX + FOLLOW

**NONE**

- NOINDEX + NOFOLLOW

※ 로봇 meta 태그도 다른 모든 HTML meta 태그와 동일하게 반드시 head 섹션에 나타나야 함

※ 로봇 meta 태그의 `name`과 `content` 값은 대소문자를 구분하지 않음

※ 서로 반대되는 지시들이 충돌되거나 중복되게 해서는 당연히 안 됨 (이에 대한 동작은 정의되어 있지 않음)

**_검색엔진 META 태그_**

- `name="robots"`인 meta 태그 외에도 다른 많은 meta 태그들을 검색엔진 로봇을 위해 사용할 수 있음

|     name=     |   content=   | 설명                                                                                          |
| :-----------: | :----------: | :-------------------------------------------------------------------------------------------- |
|  description  |  \<텍스트>   | 웹 페이지에 대한 짧은 요약 정의                                                               |
|   keywords    | \<쉼표 목록> | 웹 페이지를 기술하는 단어들을 쉼표로 구분한 목록으로, 키워드 검색을 돕기 위함                 |
| revisit-after | \<숫자 days> | 페이지가 쉽게 변경될 것이므로 지정된 날짜 이후 재방문을 지시함<br>그다지 널리 지원되지는 않음 |

<br>

## 5. 로봇 에티켓

- 1993년, 웹 로봇 커뮤니티의 개척자 Martijn Koster는 웹 로봇 제작자를 위한 가이드라인 목록을 작성했음
- 조언 중 몇 가지는 구식이 되어버렸지만, 대다수는 아직도 상당히 유용함
- [Guidelines for Robot Writers](https://www.robotstxt.org/guidelines.html)

<br>

## 6. 검색엔진

- 웹 로봇을 가장 광범위하게 사용하는 주체는 인터넷 검색엔진
- 인터넷 검색엔진은 사용자가 전 세계 어떤 주제에 대한 문서라도 찾을 수 있게끔 해줌
- 오늘날 가장 유명한 웹 사이트 중 상당수가 검색엔진
  - 많은 웹 사용자들의 시작점이 되면서 사용자들이 관심 있는 정보를 찾을 수 있도록 도와줌
- 웹 크롤러들은 검색엔진에게 문서들을 가져다줌으로써, 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 함

### 6.1 넓게 생각하라

- 수백만 명의 사용자들이 수십억 개의 웹페이지에서 원하는 정보를 찾는 상황에서,<br>복잡한 질의 엔진도 필요하고, 복잡한 크롤러도 사용해야 함
- HTTP 질의 요청이 완료되는 데에 0.5초가 걸린다면 수십억 개의 페이지들을 검색하기 위해선 5,700일이 걸림

### 6.2 현대적인 검색엔진의 아키텍처

- 오늘날 검색엔진들은 전 세계 웹 페이지들에 대해 _full-text indexes_ 라고 하는 복잡한 로컬 DB를 생성함
- 이 색인은 웹의 모든 문서에 대해 일종의 카드 카탈로그처럼 동작함

(이미지 첨부 - full-text indexes)

- 검색엔진 크롤러들은 웹페이지들을 수집해와서 full-text indexes에 추가함
- 검색엔진 사용자들은 [hotbot](http://www.hotbot.com)이나 [google](http://www.google.com) 같은 웹 검색 게이트웨이를 통해 full-text indexes에 대한 질의를 보냄
- 웹 페이지들은 매 순간 변하기 때문에 full-text indexes는 기껏해야 웹의 특정 순간에 대한 스냅샷에 불과함

### 6.3 full-text indexes

- 단어 하나를 입력받아서 해당 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스
- 데이터베이스 안의 문서들은 색인 생성 후에는 검색할 필요가 없음

### 6.4 질의 보내기

사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법

- HTML 폼을 사용자가 채워 넣고, 브라우저가 해당 폼을 HTTP GET이나 POST 요청을 통해서 게이트웨이로 보내는 식
- 게이트웨이 프로그램은 검색 쿼리를 추출하고, 해당 질의를 full-text indexes를 검색할 때 사용되는 표현식으로 변환함

### 6.5 검색 결과를 정렬하고 보여주기

- 색인을 이용한 결과가 나왔다면, 게이트웨이 애플리케이션은 최종 사용자를 위한 결과 페이지를 즉석에서 만들어냄
- 검색엔진은 결과들에 순위를 매기기 위해 똑똑한 알고리듬을 사용함
- 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타나게 하며, 이를 _relevancy ranking_ 이라고 부름
- 검색 결과의 목록에 점수를 매기고 정렬하는 과정
- 이 과정을 보조하기 위해, 많은 검색엔진이 크롤링 과정에서 수집된 통계 데이터를 실제로 사용함
- 검색엔진의 알고리듬, 크롤링 팁, 그 외 각종 기교는 검색엔진의 가장 엄격히 감추어진 비밀들

### 6.6 스푸핑

- 사용자들은 원하는 결과가 최상위 몇 줄에 보이지 않으면 불만족하므로, 검색 결과의 순서는 중요함
- 웹 마스터는 검색 결과 상단에 노출되도록 만들 만한 동기가 충분히 주어진 셈
- 반면에 검색엔진을 속이는 가짜 페이지 등을 생성해서 악용하는 케이스도 있음
- 결국 검색엔진과 로봇 구현자들은 속임수들을 더 잘 잡아내기 위해 끊임없이 relevancy ranking 알고리듬을 수정해야 함
