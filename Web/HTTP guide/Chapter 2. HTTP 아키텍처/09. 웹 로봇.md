_스스로 움직이는 사용자 에이전트_

- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 웹 사이트들을 떠돌아다니면서 컨텐츠를 가져오고 하이퍼링크를 따라가고 발견한 데이터를 처리함
- '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불림

※ 예시

- 주식시장 서버에 매 분 GET 요청을 보내서 얻은 데이터로 주가 추이 그래프를 생성하는 주식 그래프 로봇
- WWW의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
  - 웹 페이지 개수를 세고 각 페이지의 크기, 언어, 미디어 타입을 기록함
  - http://www.netcraft.com → 웹 사이트들이 어떤 서버를 사용하고 있는지 통계 자료를 수집함
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
- 상품 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹 페이지를 수집하는 가격 비교 로봇

<br>

## 1. 크롤러와 크롤링

- 웹 페이지를 가져온 뒤, 그 페이지가 가리키는 모든 웹 페이지를 가져오고, 또 그 페이지의 페이지들을 가져오는,<br>웹을 재귀적으로 반복 순회하는 로봇
- 크롤러 혹은 스파이더라고 불림
  - HTML 하이퍼링크들로 만들어진 웹을 따라 '기어다니기' 때문
- 인터넷 검색엔진은 웹을 돌아다니며 만나는 모든 문서를 끌어오기 위해 크롤러를 사용함
  - 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어짐
  - 검색 DB는 특정 단어를 포함한 문서를 찾을 수 있게 해줌
  - 찾아서 가져와야 하는 페이지들이 수십억 개에 이르다 보니 필연적으로 가장 복잡한 로봇들 중 하나가 되었음

### 1.1 어디서 시작하는가: '루트 집합'

- 굶주린 크롤러를 풀어놓기 전에 출발지점이 주어져야 함

![크롤러 루트 집합](https://user-images.githubusercontent.com/75058239/202581426-610bccd3-0b18-4fec-9cc6-21bacafe4a8b.png)

- 크롤러가 방문을 시작하는 URL들의 초기 집합을 **root set**이라고 부름
- 루트 집합을 고를 땐 관심 있는 웹 페이지들을 충분히 크롤링하기 위해 충분히 다른 장소의 URL들을 선택해야 함
- 일반적으로 웹 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없음
- 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지 목록,<br>자주 링크되지만 잘 알려져 있지 않은 페이지 목록으로 구성되어 있음
- 대규모 크롤러 제품들은 사용자들이 루트 집합에 새로운 페이지나 잘 알려지지 않은 페이지들을 추가하는 기능을 제공함
- 루트 집합은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 됨

### 1.2 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 페이지 안의 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함
- 크롤러는 간단한 HTML 파싱을 통해 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있음

### 1.3 순환 피하기

- 로봇이 웹을 크롤링할 때 루프에 빠지지 않도록 매우 조심해야 함
- 만약 A에서 출발해서, B를 거쳐서 C에 도착했는데 C에서 A로 다시 링크되면 무한 루프에 빠지게 될 것
- 이를 해결하기 위해 로봇들은 반드시 어디에 방문했었는지를 알아야 함

### 1.4 루프와 중복

루프가 크롤러에게 해로운 3가지 이유

1. 크롤러가 네트워프 대역폭을 다 차지하면서도 동일한 페이지들만을 반복해서 가져오게 될 수 있음
2. 웹 서버에 불필요한 부담을 가중시켜서 다른 사용자의 사이트 접근을 막을 수도 있게 되는데,<br>이는 서비스 방해 행위로써 법적인 문제가 될 수 있음
3. 크롤러의 애플리케이션이 쓸모없는 중복된 컨텐츠로 가득차게 됨

### 1.5 빵 부스러기의 흔적

- 불행하게도 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음
  - 동적인 게이트웨이에서 생성된 컨텐츠를 제외하더라도 수십억 개의 서로 다른 페이지들이 있기 때문
- 만약 전 세계 웹 컨텐츠의 대부분을 크롤링하고 싶다면 수십억 개의 URL을 방문할 준비가 되어 있어야 함
- 따라서 어떤 URL을 방문했는지 빠르게 판단하기 위해 복잡한 자료 구조를 사용할 필요가 있음
- 자료 구조 중에서도 빠른 속도를 위해 적어도 검색 트리나 해시 테이블이 필요할 것
- 만약 URL의 평균 길이가 40Byte이고, 대강 5억 개의 URL을 크롤링한다면 20GB 이상의 메모리가 필요함

대규모 웹 크롤러들이 사용하는 유용한 기법들

**트리와 해시 테이블**

- URL을 훨씬 빨리 찾게 해주는 소프트웨어 자료구조

**느슨한 존재 비트맵**

- 공간 최적화를 위해 몇몇 대규모 크롤러는 _presence bit array_ 같은 느슨한 자료 구조를 사용함
- 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 presence bit를 가짐
- URL이 크롤링되었을 때, 해당 URL에 대한 존재 비트가 생성됨
- 만약 존재 비트가 이미 존재한다면 크롤러는 해당 URL이 이미 크롤링되었다고 간주함
- 하지만 잠재적으로 무한한 URL 개수에 비해 유한한 존재 비트 배열을 사용하기 때문에 충돌이 발생할 수 있고,<br>충돌로 인한 패널티로 해당 URL은 크롤링에서 제외될 수 있음

**체크포인트**

- 로봇 프로그램의 갑작스런 중단에 대비해서, 방문한 URL 목록을 디스크에 저장해두고 확인함

**파티셔닝**

- 웹의 성장에 따라, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌음
- 몇몇 대규모 웹 로봇은 여러 컴퓨터의 로봇들이 동시에 일하는 _farm_ 을 운영함
- 각 로봇들은 URL들의 특정 한 부분을 담당하며, 서로를 도우며 웹을 크롤링함
  - 서로 URL들을 넘겨주거나, 오동작하는 동료를 도와주거나, 활동을 조정하기 위해 커뮤니케이션함

### 1.6 alias와 로봇 순환

- URL은 별칭을 가질 수 있기 때문에 어떤 페이지를 이미 방문했는지 구분하는 게 쉽지 않을 수 있음

※ 예시

- 포트가 생략된 URL과 생략되지 않은 URL
- `%7F`가 `~`라는 문자열을 표현하듯, 표현 방식이 다를 때
- `#`이 붙는 태그가 있긴 한데, 이에 따라 페이지가 변동되지 않는 경우
- 서버가 URL의 대소문자를 구분하지 않는 경우
- 기본 페이지 `index.html`이 생략된 경우와 생략되지 않은 경우
- IP 주소와 도메인 주소가 다르지만 같은 리소스를 가리키고 있을 때

### 1.7 URL 정규화하기

- 대부분의 웹 로봇은 '정규화'를 통해, 다른 URL이지만 같은 리소스를 가리키는 것들을 미리 제거하려고 시도함

※ URL의 정규화 방식

1. 포트가 명시되어 있지 않다면 `:80` 추가
2. `%`로 시작하는 이스케이핑 문자들은 대응되는 실제 문자들로 변환
3. `#`으로 시작하는 태그들은 제거

※ 한계

- 웹 서버가 대소문자를 구분하는지 모름
- `index.html` 같은 경로가 생략되는지 알기 위해 웹 서버의 색인 페이지 설정을 알아야 함
- IP 주소와 도메인 주소가 같은 리소스를 가리키는지 알기 위해,<br>같은 물리적 컴퓨터를 참조하는지 뿐만 아니라, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지도 알아야 함

### 1.8 파일 시스템 링크 순환

![심볼릭 링크](https://user-images.githubusercontent.com/75058239/202850924-d4f9e907-42ee-4ec3-aa1a-eb212ab97fac.jpeg)

- 파일 시스템의 심볼릭 링크는 사실상 아무것도 존재하지 않으면서, 끝없이 깊어지는 디렉토리 계층을 만들 수 있음
- 심볼릭 링크는 보통 실수에 의해 만들어지지만, 가끔 '사악한 웹 마스터'가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 함

### 1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들은 손쉽게 크롤러 루프를 만들어낼 수 있음
- 평범한 파일이지만 사실은 게이트웨이 애플리케이션인 URL을 만드는 것은 쉬운 일
- 웹 마스터에게 나쁜 뜻이 없었음에도 심볼릭 링크나 동적 컨텐츠를 통한 크롤러 함정을 만드는 경우도 있을 수 있음

### 1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법이란 없으며, 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 함
- 일반적으로 자율적인 크롤러일수록 더 쉽게 곤란한 상황에 부딪힘
- 로봇 구현자는 휴리스틱을 활용해서 문제를 피하는 데에 도움을 줄 수도 있지만,<br>이는 유효한 컨텐츠를 걸러버릴 수도 있다는 '손실'을 감수해야 하는 일

※ 웹에서 로봇이 올바르게 동작하기 위해 사용하는 기법들

**URL 정규화**

- URL을 표준 형태로 변환
- 같은 리소스를 가리키는 중복 URL 일부 회피

**너비 우선 크롤링**

- 크롤링 할 수 있는 URL들의 큰 집합을 너비 우선으로 스케줄링하면서 순환의 영향을 최소화
- 만약 로봇을 깊이 우선 방식으로 운용한다면 순환을 건드렸을 때 걷잡을 수 없게 될 것

**스로틀링**

- 특정 시간당 가져올 수 있는 페이지 수 제한

**URL 크기 제한**

- 크기 제한을 걸어둔 로봇들은 보통 1KB가 넘는 URL이라면 크롤링을 거부함
- 순환으로 인해 URL이 계속해서 길어지는 경우라면, 크기 제한으로 방지할 수 있을 것
- 이 기법을 사용하면 가져오지 못하게 되는 컨텐츠들이 분명히 있을 거라는 점에 유의해야 함

**URL/사이트 블랙리스트**

- 순환을 만들거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어서 관리하는 방법
- 사람의 손을 필요로 하지만, 오늘날 대부분의 대규모 크롤러들은 블랙리스트를 가지고 있음
- 블랙리스트는 크롤링 되는 것을 싫어하는 특정 사이트들을 피하기 위해서도 사용될 수 있음

**패턴 발견**

- 심볼릭 링크 등의 오설정들은 일정 패턴을 따르는 경향이 있음
- 예를 들면 중복 구성 요소와 함께 점점 길어질 수 있음
- 패턴이 발견되면 로봇은 URL을 잠재적 순환으로 보고 크롤링을 거절함

**컨텐츠 fingerprint**

- 복잡한 웹 크롤러들이 사용하는 '직접적인' 방법
- 이 방법을 사용하는 로봇들은 페이지의 컨텐츠에서 몇 Byte를 얻어내서 checksum을 계산함
- 만약 이전에 봤던 체크섬을 또다시 가져오게 된다면, 해당 링크는 크롤링하지 않음
- 체크섬 함수는 내용이 달라도 체크섬이 똑같게 나올 확률이 가능한 한 적은 것으로 사용해야 함
  - MD5와 같은 메시지 요약 함수가 인기 있음
- 한계
  - 동적으로 페이지가 수정되는 웹 서버들은 체크섬 계산에서 빠뜨릴 수 있음
  - 페이지 컨텐츠를 임의로 커스터마이징할 수 있는 경우, 서버의 동적인 동작이 중복 감지를 방해할 수 있음

**사람의 모니터링**

- 웹은 거친 곳, 로봇은 결국 자신이 가진 어떤 기법으로도 해결할 수 없는 문제에 봉착하게 될 것
- 모든 상용 수준 로봇은 사람이 쉽게 모니터링해서 특이사항을 바로 알 수 있도록, 진단과 로깅을 포함하도록 설계해야 함
